{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 1 -- neural machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='+2'> Words that occur insimilar contexts tend to have similar meanings.  This link between similarity in how words are distributed and similarity in  what  they  mean  is  called  <b>the distributional  hypothesis.</b> </font> (Joos (1950), Harris (1954), and Firth(1957))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='+2'>In this practice we are going to instante this linguistic hypothesis by learning representations of the meaning of words directly from their distributions in texts and using some pre-trained representations.  <it> A word’s distribution is the set of contexts in which it occurs, the neighboring words or grammatical environments.  The idea is that two words that occur in very similar distributions (that occur together with very similar words) are likely to have the same meaning. </it></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the philosopher Ludwig Wittgenstein, skeptical of the possi-bility of building a completely formal theory of meaning definitions for each word, suggested instead that “the meaning of a word is its use in the language” (Wittgen-stein, 1953, PI 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>\n",
    "    <ul>Load word embeddings</ul>\n",
    "    <ul>Train word embeddings with new corpora</ul>\n",
    "    <ul>Similarities between words</ul>\n",
    "    <ul>Term-context matrix</ul>\n",
    "    <ul>Graphic word embeddings</ul>\n",
    "    <ul>tf-idf?</ul>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec import set_parameters_to_train,train_model,load_model,get_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('cook_book_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('simmer', 0.8206007480621338),\n",
       " ('slowly', 0.7099201083183289),\n",
       " ('boil', 0.7044273614883423),\n",
       " ('ten', 0.6825717687606812),\n",
       " ('fifteen', 0.6721160411834717),\n",
       " ('longer', 0.6507183909416199),\n",
       " ('basting', 0.6453148126602173),\n",
       " ('pulpy', 0.6288616061210632),\n",
       " ('gently', 0.6045883893966675),\n",
       " ('exhibitions', 0.5933906435966492)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_words('cook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similar_by_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
